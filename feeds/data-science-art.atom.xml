<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Python, data science, and art - data science, art</title><link href="/" rel="alternate"></link><link href="/feeds/data-science-art.atom.xml" rel="self"></link><id>/</id><updated>2023-03-19T00:00:00-05:00</updated><subtitle>Data Enthusiast</subtitle><entry><title>Neural Style Transfer</title><link href="/neural-style-transfer.html" rel="alternate"></link><published>2023-03-19T00:00:00-05:00</published><updated>2023-03-19T00:00:00-05:00</updated><author><name>Heather Ann Dye</name></author><id>tag:None,2023-03-19:/neural-style-transfer.html</id><summary type="html">&lt;h2&gt;Neural Style Transfer&lt;/h2&gt;
&lt;p&gt;What is neural style transfer and how does it involve artificial intelligence?&lt;br&gt;
Neural style transfer is a technique that takes a content image and style image and combines the two images so that the final output is in the style of the &lt;em&gt;style image&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;So, for example â€¦&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Neural Style Transfer&lt;/h2&gt;
&lt;p&gt;What is neural style transfer and how does it involve artificial intelligence?&lt;br&gt;
Neural style transfer is a technique that takes a content image and style image and combines the two images so that the final output is in the style of the &lt;em&gt;style image&lt;/em&gt;.  &lt;/p&gt;
&lt;p&gt;So, for example, I can take a content image. Then select a &lt;em&gt;style&lt;/em&gt; image.
To produce a third image with the imputed style.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;Content&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Style&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Generated&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;img src="images/neural_files/squareflower.jpg" width = "200" height="200" &gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img src="/images/neural_files/picasso.jpg" width = "200" height="200" &gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img src="/images/neural_files/samplegenpicasso.jpg" width= 200 height = 200 &gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Alternatively, I can take the same photo and combine it with a style image that I created.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;Content&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Style&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Generated&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;&lt;img src="images/neural_files/squareflower.jpg" width = "200" height="200" &gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img src="/images/neural_files/YellowFlowerWaterPixel.jpg" width = "200" height="200" &gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;img src="/images/neural_files/samplegen.jpg" width= 200 height = 200 &gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this case, the &lt;em&gt;style&lt;/em&gt; was one of my photographs that I manipuled using  GIMP.  &lt;/p&gt;
&lt;p&gt;If you're interested in the art aspects, please check out my art blog
at &lt;a href="www.heatheranndye.com"&gt;www.heatheranndye.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Picasso image is &lt;a href="https://en.wikipedia.org/wiki/File:Pablo_Picasso,_1909-10,_Figure_dans_un_Fauteuil_%28Seated_Nude,_Femme_nue_assise%29,_oil_on_canvas,_92.1_x_73_cm,_Tate_Modern,_London.jpg"&gt;Figure dans un Fauteuil&lt;/a&gt;
and in the US is no longer considered to be under copyright. &lt;/p&gt;
&lt;h2&gt;Demonstration App&lt;/h2&gt;
&lt;p&gt;This technique utlizes a convolutional neural network (CNN) such as VGG-19 which has the capability to recognize 1000 object classes.  We utilitze the ability of the CNN to recognize patterns in order to perform style transfer. &lt;/p&gt;
&lt;p&gt;From an artist's perspective, this technique can utilize AI in a manner that 
does not result in copy right theft. &lt;/p&gt;
&lt;p&gt;You can construct your own neural style transfer using the (streamlit app)[https://heatheranndye-imageto-imagetostylephoto-modstream-style-jxl3h5.streamlit.app/]. 
The original source code for this project is at (https://github.com/heatheranndye/ImageToStyle)[https://github.com/heatheranndye/ImageToStyle].&lt;/p&gt;
&lt;h3&gt;References and motivation&lt;/h3&gt;
&lt;p&gt;The original motivation for this project was to explore &lt;a href="https://fastapi.tiangolo.com/"&gt;Fast Api&lt;/a&gt;  and the diffusers at &lt;a href="huggingface.co"&gt;Hugging Face&lt;/a&gt; during my 
participation in the the &lt;a href="https://pybit.es/catalogue/the-pdm-program/"&gt;PyBites Developer Mindset Program&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;I used these references to construct the application. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="www.arxiv.org/pdf/1508.06576.pdf"&gt;A Neural Algorithm of Artistic Style&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfa"&gt;Towards Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"&gt;Pytorch Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How is Neural Style Transfer Performed?&lt;/h3&gt;
&lt;p&gt;We successively apply the layers of the CNN to the content and style images; then harvest the output layers immediately prior to the max pooling. (These output layers are matrices.) In the case of the VGG19 model, there are five such layers. 
The idea in the CNN is that each successive layer captures more complicated characteristics of the image. &lt;/p&gt;
&lt;p&gt;The generated image is initially  set equal to the content image and then is modified to take on characteristics of the style image. &lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(C^k _{ij}\)&lt;/span&gt; denote the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th output layer of the content image, 
&lt;span class="math"&gt;\(G^k _{ij}\)&lt;/span&gt; denote the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th output of the generated image, and &lt;span class="math"&gt;\(A^k_{ij}\)&lt;/span&gt; denotes the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th  output of the style file.
In the selected model, there are five possible layers to harvest occuring before max pooling.  &lt;/p&gt;
&lt;p&gt;For the content layers, we let &lt;span class="math"&gt;\(O_c\)&lt;/span&gt; denote the output layers selected to optimize the content. For the style layers, we let &lt;span class="math"&gt;\(O_s\)&lt;/span&gt; denote the output layers selected from the selected from the style image. 
We need to select at least one layer of each type, keeping in mind that the later layers capture more sophisticated characteristics. &lt;/p&gt;
&lt;p&gt;We denote the content loss function as &lt;span class="math"&gt;\(\mathit{L}_c $. Then  
&lt;div class="math"&gt;$$ \mathit{L}_c (O_c) = \sum_{k \in O_c} (C^k _{ij} - G^k _{ij})^2 .$$&lt;/div&gt;
(The Pytorch model uses MSELoss, but I'm omitted the denominator for clarity.)
Next, the style loss function is modeled as 
&lt;div class="math"&gt;$$\mathit{L}_s (O_s) = \sum_{k \in O_s}
[(A^k _{ij})^T (A^k _{ij})  -  (G^k _{ij})^T (G^k _{ij})]^2 .$$&lt;/div&gt;&lt;br&gt;
We sum these two functions to obtain the total loss:
&lt;div class="math"&gt;$$\mathit{L} = \mathit{L}_s + \mathit{L}_c.$$&lt;/div&gt;
Using this loss function, we alter the generated image, so that $G^k _{ij}\)&lt;/span&gt; also changes to move the total loss towards zero. &lt;br&gt;
Here is a code snippet. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; &lt;span class="n"&gt;style_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;counter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="c1"&gt;# print(f&amp;quot;{counter}&amp;quot;)&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_feat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
        &lt;span class="n"&gt;genitem&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gen_feat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;styleitem&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;style_feat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;counter&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;genitem&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;genitem&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;styleitem&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="n"&gt;styleitem&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;style_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mse_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;style_loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice the choice of &lt;span class="math"&gt;\(A^T A\)&lt;/span&gt; in the style loss function. This ensures that the loss function has some flexibility as explained below. &lt;/p&gt;
&lt;h5&gt;Why are the loss functions different?&lt;/h5&gt;
&lt;p&gt;I don't know how the authors of the paper selected the different loss functions. But I can reflect on 
the why the choice was made. 
Let's consider the simplest version of our problem. 
We have a &lt;em&gt;picture&lt;/em&gt; consisting of a single pixel. Let's assume grayscale for simplicity. 
Our content image is represented by &lt;span class="math"&gt;\((0)\)&lt;/span&gt; and our style image is represented by &lt;span class="math"&gt;\((1)\)&lt;/span&gt;.  Our goal is to generate a third image that minimizes the distance between &lt;span class="math"&gt;\((0)\)&lt;/span&gt; and &lt;span class="math"&gt;\((1)\)&lt;/span&gt;. Remember that our generated image is initially  the content image, so we'll represent the content image as &lt;span class="math"&gt;\(h\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Then our loss function is 
&lt;span class="math"&gt;\(L = (h)^2 + (1-h)^2\)&lt;/span&gt;. Rewriting this loss function, we find that 
&lt;span class="math"&gt;\(L =  2h^2 + 1 - 2h\)&lt;/span&gt;, meaning that for this simplistic case the minimum is at the halfway point (&lt;span class="math"&gt;\(1/2\)&lt;/span&gt;) between the two values.&lt;/p&gt;
&lt;p&gt;Now let's consider a function that utilizes a transpose in conjunction with the style image.
The product of a single entry matrix and its transpose is a square, so&lt;br&gt;
&lt;span class="math"&gt;\(L = h^2 + (1^2-h^2)^2\)&lt;/span&gt; and we see that the minimum is &lt;span class="math"&gt;\(\frac{1}{\sqrt{2}}\)&lt;/span&gt;. So this result is a bit closer to the content image than the style image.&lt;/p&gt;
&lt;p&gt;In practice, we'll be working with large matrices and the interesting part is matrix multiplication is not unique - that is if &lt;span class="math"&gt;\(A^T A = B\)&lt;/span&gt;, there may be more than one solution for &lt;span class="math"&gt;\(A\)&lt;/span&gt;. 
The matrix multiplication will mean that the actual loss function for style will be more complicated and have more flexibility. &lt;/p&gt;
&lt;h3&gt;Training the Model&lt;/h3&gt;
&lt;p&gt;Once the output layers are selected, you can train the model using the code below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gen_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;generated_image&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;content_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;gen_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;style_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;style_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;content_layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# optimize the pixel values of the generated&lt;/span&gt;
        &lt;span class="c1"&gt;# image and backpropagate the loss&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;total_loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="c1"&gt;# print the image and save it after each 100 epoch&lt;/span&gt;
        &lt;span class="c1"&gt;# if e / 100:&lt;/span&gt;
        &lt;span class="c1"&gt;# print(f&amp;quot;e: {e}, loss:{total_loss}&amp;quot;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Which output layers?&lt;/h3&gt;
&lt;p&gt;You'll notice that the code snippet above specifies the output layers used in the content and style loss. 
What layers were selected?  In the images that I generated, I selected the layers recommended in the original paper. For content, I selected layer 5. For the style layers, I selected all five layers. 
However, you can use the &lt;a href="https://heatheranndye-imageto-imagetostylephoto-modstream-style-jxl3h5.streamlit.app/"&gt;streamlit app&lt;/a&gt; to experiment with the layers and determine which works best.  If you'd like to experiment with your own photos, you can download the code at &lt;a href="https://github.com/heatheranndye/ImageToStyle/blob/master/ImageToStyle/photo_mod/styletransfer.py"&gt;https://github.com/heatheranndye/ImageToStyle/blob/master/ImageToStyle/photo_mod/styletransfer.py&lt;/a&gt;. &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="data science, art"></category><category term="Pytorch"></category></entry><entry><title>Temperature Quilts in Python</title><link href="/temperature-quilts-in-python.html" rel="alternate"></link><published>2023-01-31T00:00:00-06:00</published><updated>2023-01-31T00:00:00-06:00</updated><author><name>Heather Ann Dye</name></author><id>tag:None,2023-01-31:/temperature-quilts-in-python.html</id><summary type="html">&lt;h2&gt;What is a temperature quilt?&lt;/h2&gt;
&lt;p&gt;A temperature quilt displays the daily temperature data from a specific date range in a specific location. Colors are assigned to specific temperature ranges so that
the quilt (or image) is a visual record of the climate.  This app is a demonstration project for data â€¦&lt;/p&gt;</summary><content type="html">&lt;h2&gt;What is a temperature quilt?&lt;/h2&gt;
&lt;p&gt;A temperature quilt displays the daily temperature data from a specific date range in a specific location. Colors are assigned to specific temperature ranges so that
the quilt (or image) is a visual record of the climate.  This app is a demonstration project for data storytelling that I constructed in the &lt;a href="https://pybit.es/"&gt;PyBites Professional Developer Mindset Program&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;This project uses pandas, Pillow, matplotlib along with Streamlit.io to construct a data story and a work of art! 
This project has some unique constraints and requirements (including a pdf quilt pattern) so users can construct an actual quilt using a commercially available fabric.&lt;/p&gt;
&lt;h4&gt;A photo of my &lt;em&gt;constructed&lt;/em&gt; data quilt&lt;/h4&gt;
&lt;p&gt;&lt;img alt="jpg" src="/images/data_quilt_files/actualdataquilt.jpg"&gt;&lt;/p&gt;
&lt;h4&gt;Digital Quilt Mockup and Streamlit&lt;/h4&gt;
&lt;p&gt;The streamlit app is located at &lt;a href="https://h-a-dye-dataquilt-streamstreamlit-app-zwncqy.streamlit.app/"&gt;DataQuilt&lt;/a&gt;. Once you've opened the app, simply put in your US zip code and year.  Then, the app will automatically search for the closest weather station that contains a maximum amount of data for your specified year.
The information comes from the &lt;a href="https://www.noaa.gov/"&gt;National Oceanic and Atmospheric Administration&lt;/a&gt;'s (NOAA) Global Historical Climatology Network daily (GHCNd) of weather stations. Once the closest weather station with the most data is located (not all weather stations have a complete data set), the daily maximum temperature data is binned and each bin is associated with a particular color. From this, a digital mock-up of a temperature quilt and a corresponding pattern for the physical construction of the quilt is created automatically. &lt;/p&gt;
&lt;p&gt;The code is located in the github repository: &lt;a href="https://github.com/H-A-Dye/DataQuilt"&gt;H-A-DYE/DataQuilt&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;If you're interested in the art post about this project, it is located at &lt;a href="https://heatheranndye.com"&gt;Heather Ann Dye Art&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Where to get the data&lt;/h3&gt;
&lt;p&gt;In the dataquilt package, the weather_station_inv module sorts through NOAA's inventory of weather stations. 
NOAA provides an inventory of weather stations at &lt;a href="&amp;quot;https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt"&gt;https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt&lt;/a&gt;. Each line in this text file lists a weather station, along with its lattitude and longitude and years of activity. 
This module strips the information out of the text file using string methods and creates a pandas dataframe. Next, weather stations are eliminated from the dataframe based on years of availability.  Then, the provided zip code is converted into latitude and longitude (using geopy) and the distance between each weather station and the zip code is in nautical miles.  The app uses the 10 closest weather stations in the next step.&lt;/p&gt;
&lt;h3&gt;Request data from NOAA for a specific weather station&lt;/h3&gt;
&lt;p&gt;In data_from_api, data for a selected year and weather station is requested from NOAA. However,
not all weather stations have a complete set of records. The app examines the 10 closest weather stations and selects the weather station that has the most complete set of data. I used the requests package to request the data from NOAA and extract a json data which is transformed into a pandas data frame. The data frame is examined for missing values and out of the 10 nearest weather stations, the closest station with the least missing data is selected. In the next step, the app creates a digital mockup of the quilt. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;identify_missing_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data_series&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Returns the indices of nan values in a pandas Data Series.&lt;/span&gt;
&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        data_series (pd.Series): pd.Series&lt;/span&gt;
&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;        list: List of indices with nan values.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;local_array&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_series&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;local_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;local_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;local_list&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Descriptive Statistics with pandas&lt;/h3&gt;
&lt;p&gt;In image_generator,
pandas is used to organize the data into a suitable format. Using pandas'  descriptive statistics methods, I constructed a custom binning function based on the maximum and minimum &lt;em&gt;maximum&lt;/em&gt; daily temperatures from the dataset. This custom binning function sorts the data into &lt;em&gt;bins&lt;/em&gt; ennumerated from 0 to 14. Lambda functions are used to reformat the date and the binned maximum daily temperatures.  Next, the  dataframe is reshaped and missing data values are filled with a &lt;em&gt;null&lt;/em&gt; value of 15. Each numerical value from 0 to 15 corresponds to a specific color in the digital mockup. This binned dataframe is use to create the digital mockup. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt; &lt;span class="n"&gt;my_dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noaa_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DATE&lt;/span&gt;
    &lt;span class="n"&gt;datetimes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_dates&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;%Y-%m-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;months&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetimes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetimes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;noaa_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noaa_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;noaa_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noaa_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;months&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;months&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;my_levels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noaa_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TMAX&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;grade_temp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;noaa_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;noaa_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noaa_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;levels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;my_levels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;my_small&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;noaa_data&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;months&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;days&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;levels&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;my_reshape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_small&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pivot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;days&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;months&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;levels&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;my_reshape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_reshape&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;15.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;my_reshape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next step is to tabulate the number of days in each bin and record the temperature ranges and insert these values into a data frame. 
This data frame is essentially a frequency distribution table and is formated with the end user in mind. 
Streamlit.io displays both the binned dataframe and the tabulated dataframe, which are also available in the pdf pattern file. 
The binned dataframe and the Pillow package are used to create the digital  mockup of the quilt. &lt;/p&gt;
&lt;p&gt;&lt;img alt="jpg" src="/images/data_quilt_files/samplequilt.jpg"&gt;&lt;/p&gt;
&lt;h3&gt;Real fabric!&lt;/h3&gt;
&lt;p&gt;In the colors_kona module,
we construct rgb codes for real, commercially available fabric colors in the Kona cotton fabric line, a commercially available fabric line. This brings my pdf pattern into line with industry standards for quilt patterns. Quilters can purchase the listed fabrics and create a quilt identical to the digital mockup. &lt;/p&gt;
&lt;p&gt;Here is the color range for your reference. &lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/images/data_quilt_files/ColorRange.PNG"&gt;&lt;/p&gt;
&lt;p&gt;The cmyk color codes to rgb codes and created a dictionary to contain the color information. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_kona_dictionary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;colorlist&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;COLORENNUMERATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Takes a dictionary of Kona color names and number keys and returns a&lt;/span&gt;
&lt;span class="sd"&gt;    dictionary with number key and Color_Information for Pillow.&lt;/span&gt;
&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        colorlist (dict, optional): Kona color list and keys.&lt;/span&gt;
&lt;span class="sd"&gt;        Defaults to COLORENNUMERATE.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;color_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;colorlist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
        &lt;span class="n"&gt;color_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;colorlist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;local_row&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DF_KONA&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;DF_KONA&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color_name&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="n"&gt;cmyk&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;local_row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;rgb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;color_conversion_rgb&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cmyk&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;kona_info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ColorInformation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rgb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;color_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;kona_info&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;color_dict&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Streamlit Dashboard&lt;/h3&gt;
&lt;p&gt;The frontend/dashboard is constructed using Streamlit.io. Streamlit displays each step of the process outlined above. The data can be inspected in the dashboard and the download button allows users to download a pdf with all the necessary information to create a physical version of the digital mockup. The app can be used to create multiple mockups based on year and zip code.  &lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This fun demonstration project has led to several other on-going projects that I hope to blog about soon. So if you're interested in data storytelling in a community friendly way, keep following along!&lt;/p&gt;</content><category term="data science, art"></category><category term="pandas"></category><category term="pillow"></category><category term="streamlit"></category></entry></feed>